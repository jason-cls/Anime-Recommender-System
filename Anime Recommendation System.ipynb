{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Recommendation System\n",
    "In this notebook we will construct a recommendation system (RecSys) using explicit anime rating data collected from multiple users on https://myanimelist.net/. The [dataset](https://www.kaggle.com/CooperUnion/anime-recommendations-database#anime.csv) consists of item preference data from 73,516 users on 12,294 different anime TV series, movies, OVAs, etc.\n",
    "\n",
    "## Index\n",
    "\n",
    "* [1.0 Data Cleaning, EDA, Feature Transformation](#1.0-Data-Cleaning,-EDA,-Feature-Transformation)\n",
    "    * [1.1 Rating Data](#1.1-Rating-Data)\n",
    "    * [1.2 Anime Data](#1.2-Anime-Data)\n",
    "* [2.0 Evaluation](#2.0-Evaluation)\n",
    "    * [2.1 Selecting a Metric](#2.1-Selecting-a-Metric)\n",
    "    * [2.2 Methodology](#2.2-Methodology)\n",
    "    \n",
    "* [3.0 Baseline Recommender](#3.0-Baseline-Recommender)\n",
    "* [4.0 Content-Based Filtering](#4.0-Content-Based-Filtering)\n",
    "    * [4.1 Feature Normalization](#4.1-Feature-Normalization)\n",
    "    * [4.2 Making Recommendations](#4.2-Making-Recommendations)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Data Cleaning, EDA, Feature Transformation\n",
    "First we'll import our data and familiarize ourselves with it to see if any preprocessing is required. User rating data is stored in `rating.csv`, while item data is stored in `anime.csv`.\n",
    "\n",
    "In this case, we are using **explicit rating data** since the item ratings were chosen by users themselves. Another type of data that is more abundant for most recommendation problems is **implicit data**, where item ratings are inferred from user interactions. User interactions could consist of page views, purchases, likes, follows, comments, etc.\n",
    "\n",
    "### 1.1 Rating Data\n",
    "The user rating data provided has the following columns:\n",
    "- `user_id` - non identifiable unique user id.\n",
    "- `anime_id` - the anime that this user has rated.\n",
    "- `rating` - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load user rating data\n",
    "root = os.getcwd()\n",
    "user_data_path = os.path.join(root, \"data\", \"rating.csv\") \n",
    "user_ratings = pd.read_csv(user_data_path)\n",
    "\n",
    "# Initial inspection\n",
    "print(user_ratings.dtypes)\n",
    "user_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing value counts\n",
    "pd.isnull(user_ratings).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate entries\n",
    "user_ratings.drop_duplicates(subset=['user_id','anime_id'],\n",
    "                             inplace=True)\n",
    "\n",
    "user_ratings.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(user_ratings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw rating distribution\n",
    "sns.countplot(user_ratings['rating'],\n",
    "              color='lightblue').set_title('User Rating Distribution')\n",
    "plt.show()\n",
    "print(user_ratings.describe()['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the initial distribution of ratings indicates that a large proportion of users which marked the anime as 'watched' haven't actually provided a rating (as indicated by the `-1` values). The distribution of provided ratings is left/negatively skewed on a 1-10 integer scale.\n",
    "\n",
    "This is normal behavior for users and results in a sparse distribution of item ratings. We will relabel all `-1` values to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag missing ratings with 0\n",
    "user_ratings.loc[user_ratings['rating'] == -1, 'rating'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Anime Data\n",
    "The dataset of anime data is comprised of the following information:\n",
    "\n",
    "- `anime_id` - myanimelist.net's unique id identifying an anime.\n",
    "- `name` - full name of anime.\n",
    "- `genre` - comma separated list of genres for the anime.\n",
    "- `type` - movie, TV, OVA, etc.\n",
    "- `episodes` - how many episodes in this show.\n",
    "- `rating` - average rating out of 10.\n",
    "- `members` - number of community members that are in this anime's \"group\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load anime data\n",
    "anime_data_path = os.path.join(root, \"data\", \"anime.csv\") \n",
    "anime = pd.read_csv(anime_data_path)\n",
    "\n",
    "# Initial inspection\n",
    "print(anime.dtypes)\n",
    "anime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing value counts\n",
    "pd.isnull(anime).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are missing values in some of the columns, they should be cleaned up if we are going to use them as features. We will create two seperate dataframes for the anime information, one with only average rating info, and the other with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index null ratings\n",
    "idx_null_rating = pd.Index(anime['rating']).isnull()\n",
    "\n",
    "anime = anime.loc[~idx_null_rating]  # Drop null rows from 'rating'\n",
    "anime.drop_duplicates(inplace=True)  # Drop duplicates\n",
    "\n",
    "\n",
    "########################\n",
    "# Check if anime_id's are consistent between user_ratings dataframe to avoid indexing errors later\n",
    "ur_anime_ids = user_ratings['anime_id'].unique()\n",
    "all_anime_ids = anime.anime_id.values\n",
    "\n",
    "for anime_id in ur_anime_ids:\n",
    "    if anime_id not in all_anime_ids:\n",
    "        user_ratings = user_ratings.loc[~(user_ratings['anime_id']==anime_id)]\n",
    "\n",
    "for anime_id in all_anime_ids:\n",
    "    if anime_id not in ur_anime_ids:\n",
    "        anime = anime.loc[~(anime['anime_id']==anime_id)]        \n",
    "\n",
    "user_ratings = user_ratings.reset_index(drop=True)\n",
    "#########################\n",
    "\n",
    "anime.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Ratings only\n",
    "anime_ratings = anime.drop(columns=['genre','type','episodes','members'])\n",
    "\n",
    "print(pd.isnull(anime_ratings).sum())\n",
    "anime_ratings = anime_ratings.set_index('anime_id', verify_integrity=True)\n",
    "anime_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(anime_ratings['rating'],\n",
    "             color='lightgreen',\n",
    "             kde=False).set_title('Mean Anime Rating Distribution')\n",
    "plt.show()\n",
    "print(anime_ratings.describe()['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of mean anime ratings appears to be normal with a mean of 6.5 and a std dev of 1.0. This agrees with the central limit theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index null genres\n",
    "idx_null_genre = pd.Index(anime['genre']).isnull()\n",
    "\n",
    "anime_allfeatures = anime.loc[~idx_null_genre]  # Drop null rows from 'genre'\n",
    "\n",
    "\n",
    "print(pd.isnull(anime_allfeatures).sum())\n",
    "anime_allfeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleaning 'episodes' column\n",
    "# Drop rows with 'Unknown' \n",
    "idx_ep_unknown = (anime_allfeatures['episodes'] == 'Unknown')\n",
    "anime_allfeatures = anime_allfeatures.loc[~idx_ep_unknown]\n",
    "\n",
    "# To numeric\n",
    "anime_allfeatures = anime_allfeatures.astype(dtype={'episodes':'int64'})\n",
    "\n",
    "anime_allfeatures.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(14,4))\n",
    "\n",
    "# Plot type counts\n",
    "sns.countplot(anime_allfeatures['type'],\n",
    "              color='tomato',\n",
    "              ax=axes[0]).set_title('Anime Content')\n",
    "\n",
    "\n",
    "\n",
    "# Visualize Member distribution (log count)\n",
    "sns.distplot(np.log10(anime_allfeatures['members'].rename('member count (log10)')),\n",
    "             color='orchid',\n",
    "             kde=False,\n",
    "             ax=axes[1]).set_title('Community Member Count Distribution')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to process categorical features `genre` and `type`, we need to encode them numerically. We will do so by hot-encoding the categories into binary-valued columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot-encode 'type'\n",
    "anime_allfeatures = pd.get_dummies(anime_allfeatures,\n",
    "                                   columns=['type'])\n",
    "\n",
    "# Convert col from str to list\n",
    "anime_allfeatures['genre'] = anime_allfeatures['genre'].str.split(', ')\n",
    "\n",
    "# Binarize 'genre'\n",
    "mlb = MultiLabelBinarizer()\n",
    "genre_binarized = mlb.fit_transform(anime_allfeatures['genre'])\n",
    "genre_binarized = pd.DataFrame(genre_binarized, \n",
    "                               columns=mlb.classes_,\n",
    "                               index=anime_allfeatures.index)\n",
    "\n",
    "# Join result to features df\n",
    "anime_allfeatures = anime_allfeatures.join(genre_binarized, how='left')\n",
    "anime_allfeatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Genre distribution\n",
    "genre_counts = genre_binarized.sum()\n",
    "genre_counts = genre_counts.to_frame(name='counts').reset_index()\n",
    "genre_counts = genre_counts.rename(columns={'index':'genre'})\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(14,10))\n",
    "\n",
    "# Plot type counts\n",
    "sns.barplot(\n",
    "    y='genre',\n",
    "    x='counts',\n",
    "    data=genre_counts,\n",
    "    color='tab:purple',\n",
    "    ax=axes[0]\n",
    ").set_title('Genre Distribution')\n",
    "\n",
    "\n",
    "# Visualize 'episodes' distribution\n",
    "sns.distplot(np.log10(anime_allfeatures['episodes'].rename('episode count (log10)')),\n",
    "             color='tab:cyan',\n",
    "             kde=False,\n",
    "             ax=axes[1]).set_title('Episode Count Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that a large portion of anime is have *comedic* elements to them. Other popular genres include *Action*, *Adventure*, and *Fantasy*.\n",
    "\n",
    "Scaling the distribution of episode by log10, reveals that most of the anime series have less than 10 episodes associated with them likely because movies, and OVAs are included in the data. A good portion of anime have more than 10 episodes, with a few of them having more than 50 episodes.\n",
    "\n",
    "The episode counts can thus be binned into categorical features as a rough indicator of the length of the series:\n",
    "- **few**: less than 10 episodes\n",
    "- **moderate**: 10 to 50 episodes\n",
    "- **many**: more than 50 episodes\n",
    "\n",
    "Reducing the granularity of episode counts will allow for more descriptive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical 'episodes' \n",
    "def categorize_episodes(episodes_col):\n",
    "    if episodes_col < 10:\n",
    "        return 'few'\n",
    "    elif episodes_col <=50:\n",
    "        return 'moderate'\n",
    "    else:\n",
    "        return 'many'\n",
    "\n",
    "anime_allfeatures['episodes_category'] = anime_allfeatures['episodes'].apply(categorize_episodes)\n",
    "anime_allfeatures = pd.get_dummies(anime_allfeatures,\n",
    "                                   columns=['episodes_category'],\n",
    "                                   prefix='episodes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will employ a similar strategy for categorizing and encoding the `members` feature since its distribution is quite sparse. The distribution will be categorized into 5 quantiles of (approximately) equal sizes:\n",
    "- **1st quantile**: 0-20% of distribution\n",
    "- **2nd quantile**: 20-40% of distribution\n",
    "- **3rd quantile**: 40-60% of distribution\n",
    "- **4th quantile**: 60-80% of distribution\n",
    "- **5th quantile**: 60-100% of distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 'members' quintile values\n",
    "member_quantiles = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "quintile_names = ['Q1','Q2','Q3','Q4','Q5']\n",
    "\n",
    "member_quant_vals = anime_allfeatures['members'].quantile(member_quantiles,\n",
    "                                                          interpolation='nearest').values\n",
    "\n",
    "print('`members` Quintile values:', list(member_quant_vals))\n",
    "\n",
    "member_q_val_map = {}\n",
    "for i in range(len(member_quant_vals)):\n",
    "    member_q_val_map[member_quant_vals[i]] = quintile_names[i]\n",
    "\n",
    "# Apply member categories\n",
    "def categorize_members(members_col):\n",
    "    for q_val in member_quant_vals:\n",
    "        if members_col <= q_val:\n",
    "            return member_q_val_map[q_val]\n",
    "\n",
    "anime_allfeatures['members_quintile'] = anime_allfeatures['members'].apply(categorize_members)   \n",
    "\n",
    "# One-hot encode 'members' (binned into quintiles)\n",
    "anime_allfeatures = pd.get_dummies(anime_allfeatures,\n",
    "                                   columns=['members_quintile'],\n",
    "                                   prefix='members')\n",
    "\n",
    "anime_allfeatures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Evaluation\n",
    "Since recommender systems are concerned with providing good rankings of items to different users, we need a way to differentiate the quality of a RecSys' recommendations. To do so, we need a way of testing a RecSys in an offline manner with ranking metrics.\n",
    "\n",
    "While offline testing is important, the real performance of a RecSys depends on how much business value it brings once deployed online. In practice, this can be done by performing A/B testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Selecting a Metric\n",
    "To measure how good a recommendation is, we need to quantify how well the items are ranked. Popular ranking metrics include **Mean Reciprocal Rank**, **Mean Average Precision (MAP@K)**, and **Normalized Discounted Cumulative Gain (NDCG@K)**. Since we are not ranking binary items (relevant / irrelevant) in this case and we have user rating on a numbered scale, we will use NDCG@K, with K representing the top K recommendations we would make to a user. NDCG@K assumes that items in the query which appear first are more important that those further down in the list, which is suitable since we would like to recommend the most relevant items first.\n",
    "\n",
    "**Discounted Cumulative Gain (DCG@K)** of a ranked set of items is defined as:\n",
    "\n",
    "$$ DCG_K = \\sum\\limits_{i=1}^{K} \\frac{rel_i}{\\log_2{(i+1)}} $$\n",
    "\n",
    "- $rel_i$ - the true relevance of the result at position i (e.g. assigned rating score)\n",
    "- $K$ - number of elements in the recommended ranking\n",
    "\n",
    "_________________________\n",
    "\n",
    "The best possible ranking of $K$ elements is represented as the **Idealized Discounted Cumulative Gain (IDCG@K)**\n",
    "\n",
    "$$ IDCG_K = \\sum\\limits_{i=1}^{REL_K} \\frac{rel_i}{\\log_2{(i+1)}} $$\n",
    "\n",
    "- $REL_K$ = the list of all corpus items ordered by decreasing relevance up to position $K$\n",
    "\n",
    "_________________________\n",
    "\n",
    "\n",
    "To normalize the DCG@K, we simply divide it by the IDCG@K to get the **Normalized Discounted Cumulative Gain (NDCG@K)**.\n",
    "\n",
    "$$ NDCG_K = \\frac{DCG_K}{IDCG_K} $$\n",
    "\n",
    "_________________________\n",
    "\n",
    "NDCG tells us how well we've ranked items offline for a user, so we simply take the average of all queries to estimate the performance of a recommendation engine. A few edge cases to consider are the following:\n",
    "\n",
    "- When a user has no relevant documents the IDCG will be zero. In this case we need to manually set the NDCG to 0, to avoid division by zero.\n",
    "- The size of the recommended list or ideal list can be less than K, so we need to pad the size of the list with the minimum DCG of 0 to have fixed length when computing NDCG@K.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_item_rating(user_id, item_id, user_ratings):\n",
    "    '''Retrieves the rating a user has assigned to an item'''\n",
    "    account_ratings = user_ratings.loc[user_ratings['user_id'] == user_id]\n",
    "    assert account_ratings.shape[0] != 0, 'user_id does not exist or has not watched anything'\n",
    "    try:\n",
    "        #print(user_id, item_id)\n",
    "        return account_ratings.loc[account_ratings['anime_id']==item_id, 'rating'].values[0]\n",
    "    except IndexError:\n",
    "        return 0  # Case where user has not watched the anime\n",
    "    \n",
    "\n",
    "# Define DCG\n",
    "def DCG_K(ranked_ids, user_id, K, user_ratings):\n",
    "    '''Computes Discounted Cumulative Gain for a user'''\n",
    "    assert len(ranked_ids) <= K, 'Length of provided rankings greater than K'\n",
    "\n",
    "    pad_value = 0\n",
    "    dcg = 0\n",
    "    \n",
    "    for idx in range(K):\n",
    "        i = idx+1\n",
    "        try:\n",
    "            item_id = ranked_ids[idx]\n",
    "            rel = get_user_item_rating(user_id, item_id, user_ratings)\n",
    "        except IndexError:\n",
    "            rel = pad_value  # Pad if reqd\n",
    "        dcg += rel / np.log2(i+1)       \n",
    "    return dcg\n",
    "        \n",
    "\n",
    "# Define NDCG@K metric\n",
    "def NDCG_K(recommendations, ideal_recs, user_id, K, user_ratings):\n",
    "    '''Computes Normalized Discounted Cumulative Gain for a user/query'''\n",
    "    \n",
    "    # Compute DCG / IDCG\n",
    "    dcg = DCG_K(recommendations, user_id, K, user_ratings)\n",
    "    idcg = DCG_K(ideal_recs, user_id, K, user_ratings)\n",
    "    \n",
    "    if idcg == 0:\n",
    "        return 0  #Avoid div by zero\n",
    "    else:\n",
    "        return dcg/idcg \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Methodology\n",
    "Unlike the offline evaluation of traditional machine learning algorithms, data cannot be separated into independent training and testing sets. This is because recommenders typically need to have knowledge of a user during training to make recommendations for them during testing. The same holds true for items which can be recommended. Due to this, all users and items must be included during training and testing.\n",
    "\n",
    "To create the testing set, we will use all of the user rating data as is. The training set however will consist of a proportion of user ratings randomly masked (removed). If the rating data was timestamped, we could mask the ratings input after a certain date instead of randomly. The goal of a RecSys then is to recommend the masked ratings as good recommendations for a user. A validation set can be created by doing the same to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rating_matrix(user_ratings_df, anime_df):\n",
    "    '''RETURNS:\n",
    "    - sparse rating matrix,\n",
    "    - user id to matrix row index mapping\n",
    "    - matrix row index to user id mapping\n",
    "    - anime id to matrix col index mapping\n",
    "    - matrix col index to anime id mapping\n",
    "    '''\n",
    "    # User and Anime ID's aren't in perfect chronological order\n",
    "    # Map iloc to loc index so matrix indices can be mapped to ID's later\n",
    "    rating_matrix_idx_user_id_map = user_ratings_df['user_id'].drop_duplicates().reset_index(drop=True).to_dict()\n",
    "    user_id_rating_matrix_idx_map = {v: k for k, v in rating_matrix_idx_user_id_map.items()} #inverse map\n",
    "    \n",
    "    # Map iloc to loc index for anime_id\n",
    "    rating_matrix_idx_anime_id_map = anime_df.reset_index()['anime_id'].to_dict()\n",
    "    anime_id_rating_matrix_idx_map = {v: k for k, v in rating_matrix_idx_anime_id_map.items()} #inverse\n",
    "    \n",
    "    # Construct rating matrix (sparse format)\n",
    "    n_users = len(rating_matrix_idx_user_id_map.keys())\n",
    "    n_items = len(rating_matrix_idx_anime_id_map.keys())\n",
    "    mat_shape = (n_users, n_items)   \n",
    "    ratings = []\n",
    "    row_idx = []\n",
    "    col_idx = []\n",
    "    nonzero_user_ratings = user_ratings_df.loc[user_ratings_df['rating']!=0]\n",
    "\n",
    "    for row_tuple in nonzero_user_ratings.itertuples():\n",
    "        i = user_id_rating_matrix_idx_map[row_tuple.user_id]\n",
    "        j = anime_id_rating_matrix_idx_map[row_tuple.anime_id]\n",
    "        row_idx.append(i)\n",
    "        col_idx.append(j)\n",
    "        ratings.append(row_tuple.rating)\n",
    "        \n",
    "    rating_matrix = csr_matrix((ratings, (row_idx, col_idx)),\n",
    "                               shape=mat_shape)\n",
    "    \n",
    "    return (rating_matrix,\n",
    "            user_id_rating_matrix_idx_map,\n",
    "            rating_matrix_idx_user_id_map,\n",
    "            anime_id_rating_matrix_idx_map,\n",
    "            rating_matrix_idx_anime_id_map)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Evaluation Methods\n",
    "def train_test_mask(user_ratings_df, anime_df, mask_frac=0.2, random_state=1):\n",
    "    '''RETURNS:\n",
    "    - masked user_ratings_df\n",
    "    - train user_ratings_df\n",
    "    - test user_ratings_df\n",
    "    - train rating matrix\n",
    "    - test rating matrix\n",
    "    - rating matrix index mapping info\n",
    "    '''\n",
    "    np.random.seed(random_state) #set random state\n",
    "    \n",
    "    nz_user_ratings = user_ratings_df.loc[user_ratings_df.rating != 0]\n",
    "    nz_ur_index = nz_user_ratings.index\n",
    "    n_mask = int(mask_frac * len(nz_ur_index))\n",
    "    \n",
    "    # randomly mask non-zero ratings from user_ratings_df to get train user_ratings_df\n",
    "    mask_idx = np.random.choice(nz_ur_index, n_mask, replace=False)\n",
    "    masked_user_ratings_df = user_ratings_df.loc[mask_idx]\n",
    "    \n",
    "    train_user_ratings_df = user_ratings_df.copy()\n",
    "    train_user_ratings_df.loc[mask_idx, 'rating'] = 0\n",
    "\n",
    "    test_user_ratings_df = user_ratings_df\n",
    "          \n",
    "    # get train rating matrix, w/ mapping info\n",
    "    (rating_matrix_TRAIN,\n",
    "    user_id_rating_matrix_idx_map,\n",
    "    rating_matrix_idx_user_id_map,\n",
    "    anime_id_rating_matrix_idx_map,\n",
    "    rating_matrix_idx_anime_id_map) = construct_rating_matrix(train_user_ratings_df, anime_df)\n",
    "    \n",
    "    # get test rating matrix, w/ mapping info\n",
    "    (rating_matrix_TEST, _, _,  _, _) = construct_rating_matrix(test_user_ratings_df, anime_df)\n",
    "    \n",
    "    return (masked_user_ratings_df,\n",
    "            train_user_ratings_df,\n",
    "            test_user_ratings_df,\n",
    "            rating_matrix_TRAIN,\n",
    "            rating_matrix_TEST,\n",
    "            user_id_rating_matrix_idx_map,\n",
    "            rating_matrix_idx_user_id_map,\n",
    "            anime_id_rating_matrix_idx_map,\n",
    "            rating_matrix_idx_anime_id_map)\n",
    "    \n",
    "\n",
    "def get_ideal_recommendations(masked_user_ratings, user_id, K_max):\n",
    "    '''Retrieves the best possible ranking of ratings for a user which were masked in the training data'''\n",
    "    user_mask = masked_user_ratings.loc[masked_user_ratings['user_id']==user_id]\n",
    "    user_mask = user_mask.sort_values('rating', ascending=False)\n",
    "    \n",
    "    # Limit max number of recommendations to K_max\n",
    "    if user_mask.shape[0] > K_max:\n",
    "        user_mask = user_mask.iloc[:K_max]\n",
    "    \n",
    "    return list(user_mask['anime_id'].values)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_RecSys(model, masked_user_ratings, all_user_ratings, K=10):\n",
    "    '''Computes the average NDCG@K over all users with ratings masking in the training data'''\n",
    "    masked_users = masked_user_ratings['user_id'].unique()\n",
    "    n_masked_users = len(masked_users)\n",
    "    NDCG_sum = 0\n",
    "    \n",
    "    for user_id in tqdm(masked_users):\n",
    "        model_recs = model.recommend(user_id, K)\n",
    "        ideal_recs = get_ideal_recommendations(masked_user_ratings, user_id, K)\n",
    "        NDCG_sum += NDCG_K(model_recs, ideal_recs, user_id, K, all_user_ratings)\n",
    "        \n",
    "    return NDCG_sum / n_masked_users\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(masked_user_ratings_df,\n",
    "train_user_ratings_df,\n",
    "test_user_ratings_df,\n",
    "rating_matrix_TRAIN,\n",
    " rating_matrix_TEST,\n",
    "user_id_rating_matrix_idx_map,\n",
    "rating_matrix_idx_user_id_map,\n",
    "anime_id_rating_matrix_idx_map,\n",
    "rating_matrix_idx_anime_id_map) = train_test_mask(user_ratings, anime_ratings, mask_frac=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Baseline Recommender\n",
    "A simple, often hard to beat baseline recommender is one which always recommends the most popular items which the user has not yet seen. We will use this naive popularity recommender to compare the relative performance of other more complex recommomender systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map anime_id to descriptive info\n",
    "anime_infomap = anime[['anime_id','name','genre','type','episodes','rating','members']] \\\n",
    ".set_index('anime_id', verify_integrity=True) \\\n",
    ".sort_index() \\\n",
    ".transpose().to_dict('index')\n",
    "\n",
    "## Utility functions ##\n",
    "def get_watched(userID, user_ratings, masked_user_ratings=None):\n",
    "    '''Returns anime which a user has already watched'''\n",
    "    target_user_ratings = user_ratings.loc[user_ratings['user_id'] == userID]\n",
    "    watched = set(target_user_ratings['anime_id'].values)\n",
    "    \n",
    "    # Filter any masked ratings provided (TRAIN / TEST EVALUATION PURPOSES)\n",
    "    if masked_user_ratings is not None:\n",
    "        anime_id_toMask = set(masked_user_ratings.loc[user_ratings['user_id']==userID, 'anime_id'].values)\n",
    "        watched = watched - anime_id_toMask  #diff of sets\n",
    "        \n",
    "    if len(watched) == 0:\n",
    "        print(\"user_id {0} does not exist or has not recorded anything as 'watched'\".format(userID))\n",
    "        \n",
    "    return list(watched)\n",
    "\n",
    "\n",
    "\n",
    "def tabulate_id_info(recommendations):\n",
    "    '''Returns user recommendation list in a dataframe'''\n",
    "    n_items = len(recommendations)\n",
    "    rankings = np.arange(n_items) + 1\n",
    "    \n",
    "    names = []\n",
    "    genres = []\n",
    "    types = []\n",
    "    episodes = []\n",
    "    ratings = []\n",
    "    members = []\n",
    "    for anime_id in recommendations:\n",
    "        try:\n",
    "            names.append(anime_infomap['name'][anime_id])\n",
    "            genres.append(anime_infomap['genre'][anime_id])\n",
    "            types.append(anime_infomap['type'][anime_id])\n",
    "            episodes.append(anime_infomap['episodes'][anime_id])\n",
    "            ratings.append(anime_infomap['rating'][anime_id])\n",
    "            members.append(anime_infomap['members'][anime_id])\n",
    "        except KeyError:\n",
    "            # if anime_id exists, but no info is available from the data provided\n",
    "            names.append('n/a')\n",
    "            genres.append('n/a')\n",
    "            types.append('n/a')\n",
    "            episodes.append('n/a')\n",
    "            ratings.append('n/a')\n",
    "            members.append('n/a')\n",
    "        \n",
    "    data = {'anime id':recommendations,\n",
    "            'name': names,\n",
    "            'genre': genres,\n",
    "            'type': types,\n",
    "            'episodes': episodes,\n",
    "            'avg rating': ratings,\n",
    "            'members': members}\n",
    "    \n",
    "    recs_df = pd.DataFrame(data=data, index=rankings)\n",
    "    return recs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class popularRecommender:\n",
    "    '''Popularity recommender instance'''\n",
    "    \n",
    "    def __init__(self, user_ratings, ratings_mask=None):\n",
    "        self.ratings = user_ratings\n",
    "        self.popular_ranking = np.array([]) #initialize as empty\n",
    "        self.ratings_mask = ratings_mask\n",
    "        \n",
    "    def train(self):\n",
    "        '''Ranks popularity of each anime(id) chronologically'''\n",
    "        self.popular_ranking = self.ratings['anime_id'].value_counts().index\n",
    "        \n",
    "    def recommend(self, user_id, k):\n",
    "        '''Recommends top k items to a user'''\n",
    "        assert len(self.popular_ranking) != 0, 'Cannot recommend. Training required.'\n",
    "        watched = get_watched(user_id, self.ratings, self.ratings_mask) # fetch seen items\n",
    "        recommendations = []\n",
    "        \n",
    "        for anime_id in self.popular_ranking:\n",
    "            if anime_id not in watched:\n",
    "                recommendations.append(anime_id)\n",
    "                \n",
    "            if len(recommendations) == k:\n",
    "                return recommendations\n",
    "        \n",
    "        # if ranking list is exhausted for large k\n",
    "        print('k is larger than the number of possible recommendations.\\n\\\n",
    "Recommendations with Cardinality less than k returned')\n",
    "        return recommendations\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our popularity based recommender defined, we can retrieve sample top 10 recommendation for a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 recommendations for user 1 - POPULARITY RECOMMENDER\n",
    "popular_recsys = popularRecommender(user_ratings)\n",
    "popular_recsys.train()\n",
    "user_1_recs = popular_recsys.recommend(user_id=1, k=10)\n",
    "tabulate_id_info(user_1_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate the performance score (mean NDCG) of the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model\n",
    "results_path = 'modelTestResults.csv'\n",
    "\n",
    "popular_recsys = popularRecommender(train_user_ratings_df, masked_user_ratings_df)\n",
    "popular_recsys.train()\n",
    "model_name = 'Popularity Recommender (baseline)'\n",
    "\n",
    "# Load results if saved\n",
    "try:\n",
    "    model_results = pd.read_csv(results_path) \n",
    "except FileNotFoundError:\n",
    "    model_results = pd.DataFrame(columns=['name', 'Mean NDCG (K=10)'])\n",
    "    model_results.to_csv(results_path)\n",
    "\n",
    "    \n",
    "if model_name not in model_results['name'].values:\n",
    "    mean_NDCG = evaluate_RecSys(popular_recsys, masked_user_ratings_df, test_user_ratings_df, K=10)   \n",
    "    \n",
    "    row = [model_name, mean_NDCG]\n",
    "    model_results.loc[len(model_results)] = row\n",
    "    model_results.to_csv(results_path)\n",
    "\n",
    "\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Content-Based Filtering\n",
    "Recommender systems (RecSys) are typically grouped into two main categories: **content-based filtering**, and **collaborative filtering**. Content-based filtering RecSys aim to provide recommendations to a user based on the items that user has already interacted with. If a user has highly rated an item in the past, the content-based filtering will learn that user's preferences and recommend items matching the user's preferences and past interactions. \n",
    "\n",
    "To do this, we will need to encode items in a feature space. A user must also be able to be represented in this same feature space in order to capture his/her item preferences. A similarity measure could then be used to capture how relevant any item is to a user for personalized recommendations.\n",
    "\n",
    "Like any model, it has advantages and disadvantages:\n",
    "- **Advantages**\n",
    "    - Does not require data of other users to make recommendations.\n",
    "    - Good at capturing a user's niche item interests few people have interest in.\n",
    "- **Disadvantages**\n",
    "    - Lack of variety - the model will always recommend items similar to what the user has past interaction with, and will never recommend different types of items the user does not know about, but may like.\n",
    "    - Model performance is heavily dependent on good feature engineering in order to represent items and users in the same feature space. Domain knowledge is often essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve item feature representations\n",
    "anime_features = anime_allfeatures.set_index('anime_id', verify_integrity=True)\n",
    "anime_features = anime_features.drop(columns=['name','genre','episodes','rating','members'])\n",
    "anime_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `rating` feature was dropped because content based filtering makes recommendations regardless of other users' input.\n",
    "\n",
    "We need a rating matrix to represent each user's item preferences. A sparse matrix representation will be used for the rating matrix since there are a lot of users and items in the system. Most users have only seen a few of the whole collection of items, so the representation is more memory efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if anime_id's are consistent in user_ratings dataframe to avoid indexing errors later\n",
    "ur_anime_ids = user_ratings['anime_id'].unique()\n",
    "af_anime_ids = anime_features.index.values\n",
    "\n",
    "cbf_user_ratings = user_ratings.copy()\n",
    "\n",
    "for anime_id in ur_anime_ids:\n",
    "    if anime_id not in af_anime_ids:\n",
    "        cbf_user_ratings = cbf_user_ratings.loc[~(cbf_user_ratings['anime_id']==anime_id)]\n",
    "        \n",
    "for anime_id in af_anime_ids:\n",
    "    if anime_id not in ur_anime_ids:\n",
    "        anime_features = anime_features.drop(anime_id) \n",
    "        \n",
    "# user ratings for content based filtering        \n",
    "cbf_user_ratings = cbf_user_ratings.reset_index(drop=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different split for content based filtering using anime_features dataframe\n",
    "(cbf_masked_user_ratings_df,\n",
    "cbf_train_user_ratings_df,\n",
    "cbf_test_user_ratings_df,\n",
    "cbf_rating_matrix_TRAIN,\n",
    "cbf_rating_matrix_TEST,\n",
    "cbf_user_id_rating_matrix_idx_map,\n",
    "cbf_rating_matrix_idx_user_id_map,\n",
    "cbf_anime_id_rating_matrix_idx_map,\n",
    "cbf_rating_matrix_idx_anime_id_map) = train_test_mask(cbf_user_ratings, anime_features, mask_frac=0.2, random_state=1)\n",
    "\n",
    "print(type(cbf_rating_matrix_TRAIN))\n",
    "print(cbf_rating_matrix_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw feature matrix\n",
    "feature_matrix = anime_features.to_numpy(copy=True)\n",
    "print(feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Feature Normalization\n",
    "\n",
    "With our user ratings matrix and feature matrix established, the next step would be to normalize each anime's binary features by the square root of the number of flagged features (flagged with 1). The normalization is necessary to avoid weighting items with more flagged features much heavier than those with less features.\n",
    "\n",
    "Similiarly, we would want to avoid weighing some features more heavily than others, so we can also normalize each feature by the **inverse document frequency (IDF)**. Here our 'documents' would be users in this context.\n",
    "\n",
    "IDF is computed as,\n",
    "\n",
    "$\n",
    "IDF = \\log(\\frac{N}{DF})\n",
    "$\n",
    "\n",
    "- **N** = total number of documents (users) in the corpus\n",
    "- **DF** = document frequency, the number of documents where the term (feature) appears in the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Making Recommendations\n",
    "\n",
    "Recall that in order to recommend items to a user, we first need to represent the user in the same space as the items. In other words, we need to represent each user using the same features we've used to describe each item.\n",
    "\n",
    "Assuming ***R*** represents the rating matrix for all users and items, and ***F*** is the feature matrix for each item, the user representation matrix ***U*** can be simply derived as follows, where each row represents a user's profile vector:\n",
    "\n",
    "$\n",
    "U = R \\cdot F\n",
    "$\n",
    "\n",
    "To determine if a user's preferences align with an item, we need to compute a similarity metric. While many similarity metrics exist (cosine similarity, pearson correlation, etc.), we will chose the **dot product** as our metric in this case, with larger values indicating higher similarity.\n",
    "\n",
    "$\n",
    "similarity = u \\cdot i\n",
    "$\n",
    "- **u** = a user's profile vector\n",
    "- **i** = an item's feature representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBFRecommender:\n",
    "    '''Content-Based Filtering Recommender'''\n",
    "    \n",
    "    def __init__(self, rating_matrix, feature_matrix,\n",
    "                 user_id_rating_matrix_idx_map,\n",
    "                 rating_matrix_idx_user_id_map,\n",
    "                 anime_id_rating_matrix_idx_map,\n",
    "                 rating_matrix_idx_anime_id_map,\n",
    "                 ratings_mask=None):\n",
    "        \n",
    "        # Normalize each item by number of features\n",
    "        norm_vec = np.sqrt(np.sum(feature_matrix, axis=1))\n",
    "        norm_feature_matrix = feature_matrix / norm_vec.reshape(-1,1)\n",
    "\n",
    "        # Normalize each feature by with idf of the feature\n",
    "        tf = np.sum(feature_matrix, axis=0)\n",
    "        idf = np.log10(rating_matrix.shape[0] / tf)\n",
    "        norm_feature_matrix = norm_feature_matrix * idf.reshape(1,-1)        \n",
    "        \n",
    "        self.F = csr_matrix(norm_feature_matrix) # normalized, sparse\n",
    "        self.R = rating_matrix\n",
    "        self.U = None   #stores user profiles\n",
    "        \n",
    "        self.uid_rmi_map = user_id_rating_matrix_idx_map\n",
    "        self.rmi_uid_map = rating_matrix_idx_user_id_map\n",
    "        self.aid_rmi_map = anime_id_rating_matrix_idx_map\n",
    "        self.rmi_aid_map = rating_matrix_idx_anime_id_map\n",
    "        self.ratings_mask = ratings_mask\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        '''Compute all user profile vectors'''\n",
    "        self.U = np.dot(self.R, csr_matrix(self.F))\n",
    "        \n",
    "    def recommend(self, user_id, k):\n",
    "        '''Recommends top k items to a user'''\n",
    "        user_idx = self.uid_rmi_map[user_id]\n",
    "        user_vec = self.U[user_idx]\n",
    "        \n",
    "        # dot product similarity score\n",
    "        item_scores = np.dot(user_vec, self.F.T)\n",
    "        item_scores = np.ravel(item_scores.todense())\n",
    "        \n",
    "        # Sort descending\n",
    "        sorted_item_idx = np.argsort(item_scores)[::-1]\n",
    "               \n",
    "        # filter out already seen\n",
    "        watched = get_watched(user_id, user_ratings, self.ratings_mask) # fetch seen items\n",
    "        recommendations = []\n",
    "              \n",
    "        for item_idx in sorted_item_idx:\n",
    "            candidate_anime_id = self.rmi_aid_map[item_idx]\n",
    "            \n",
    "            if candidate_anime_id not in watched:\n",
    "                recommendations.append(candidate_anime_id)\n",
    "                \n",
    "            if len(recommendations) == k:\n",
    "                return recommendations\n",
    "            \n",
    "        # if ranking list is exhausted for large k\n",
    "        print('k is larger than the number of possible recommendations.\\n\\\n",
    "Recommendations with Cardinality less than k returned')\n",
    "        return recommendations\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all of the given data, let's make a few sample recommendations for a user with our content-based filtering RecSys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 recommendations for user_id=10\n",
    "cbf = CBFRecommender(cbf_rating_matrix_TEST, feature_matrix,\n",
    "                     cbf_user_id_rating_matrix_idx_map,\n",
    "                     cbf_rating_matrix_idx_user_id_map,\n",
    "                     cbf_anime_id_rating_matrix_idx_map,\n",
    "                     cbf_rating_matrix_idx_anime_id_map)\n",
    "cbf.train()\n",
    "recommendations = cbf.recommend(user_id=10, k=10)\n",
    "tabulate_id_info(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendations above can then be compared to the anime already watched by the user, listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already watched anime for user_id=10\n",
    "tabulate_id_info(get_watched(userID=10, user_ratings=cbf_user_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief inspection reveals that the recommendations made are suitable based on the user's past interactions with similar genres.\n",
    "\n",
    "Now let's evaluate the quality of the model's offline recommendations against the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate content-based filtering model\n",
    "cbf_recsys = CBFRecommender(cbf_rating_matrix_TRAIN, feature_matrix,\n",
    "                            cbf_user_id_rating_matrix_idx_map,\n",
    "                            cbf_rating_matrix_idx_user_id_map,\n",
    "                            cbf_anime_id_rating_matrix_idx_map,\n",
    "                            cbf_rating_matrix_idx_anime_id_map,\n",
    "                            cbf_masked_user_ratings_df)\n",
    "\n",
    "cbf_recsys.train()\n",
    "model_name = 'Content-Based Filtering'\n",
    "\n",
    "# Load results if saved\n",
    "try:\n",
    "    model_results = pd.read_csv(results_path) \n",
    "except FileNotFoundError:\n",
    "    model_results = pd.DataFrame(columns=['name', 'Mean NDCG (K=10)'])\n",
    "    model_results.to_csv(results_path)\n",
    "\n",
    "    \n",
    "if model_name not in model_results['name'].values:\n",
    "    mean_NDCG = evaluate_RecSys(cbf_recsys, cbf_masked_user_ratings_df, cbf_test_user_ratings_df, K=10)   \n",
    "    \n",
    "    row = [model_name, mean_NDCG]\n",
    "    model_results.loc[len(model_results)] = row\n",
    "    model_results.to_csv(results_path)\n",
    "\n",
    "\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://developers.google.com/machine-learning/recommendation\n",
    "- https://www.analyticsvidhya.com/blog/2015/08/beginners-guide-learn-content-based-recommender-systems/\n",
    "- http://fastml.com/evaluating-recommender-systems/\n",
    "- https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
